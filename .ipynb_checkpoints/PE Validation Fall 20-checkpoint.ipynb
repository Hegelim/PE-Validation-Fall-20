{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PE Validation Fall 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules \n",
    "# please run this everytime you open it\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib as plt \n",
    "import seaborn as sns\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out a way to read in schema txt files (inside the schema folder)\n",
    "# for example, I have a source file (such as Source_relevance.csv) and \n",
    "# I know the question_label, but I want to build a function to know \n",
    "# whether this question (Q1/Q2..) is multiple-choice or checklist\n",
    "# build a function to accomplish this\n",
    "# func(schemafile(...txt), input(..csv), question_label(Q1/Q2...)) -> checlist/multiple choice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not only do I want to know whether it's multiple-choice or checklist \n",
    "# I also want to know how many different answer choices are there\n",
    "# build a function to accomplish this \n",
    "# func(schemafile(...txt), input(..csv), question_label(Q1/Q2...)) -> # of different answer choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out how to uniquely identify each submission, \n",
    "# since there might be one contributor who made \n",
    "# several submissions. \n",
    "# can't use contributor_uuid. What to use? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot-encode checklist questions to [0, 1]\n",
    "# for example, Q4 in source relevance task is a checklist question\n",
    "# if someone selects both A1 and A2, then it's [1, 1]\n",
    "# return a dataframe that has each question_label but list out the answers in [] format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation ground below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charlie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Katherine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yewen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read json lines file \n",
    "# data = pd.read_json('2020 data/2020-09-25_pe_webhooks.jsonl', lines=True)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source[source[\"contributor_uuid\"] == \"00f548b7-6b63-4b47-828e-8e416b6ca0e2\"][[\"contributor_uuid\", \"created\", \"finish_time\", \"elapsed_seconds\", \"question_label\", \"answer_label\", \"answer_uuid\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on here? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do about multi-choice? <br> \n",
    "What other questions we are expected to address? <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K's algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now only try working with multiple-choice nominal questions <br>\n",
    "Example, Q2 in Source Relevance Specialist task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select rows that correspond to question 2 \n",
    "# ques_two_b = source[source[\"question_label\"] == \"T1.Q2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ques_two_b.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # experiment to find unique identifier \n",
    "# ex = source[source[\"contributor_uuid\"] == \"00f548b7-6b63-4b47-828e-8e416b6ca0e2\"]\n",
    "# ex = ex.loc[:, [\"article_filename\", \"quiz_task_uuid\", \"contributor_uuid\", \"tua_uuid\", \"question_label\", \"answer_label\", \"submitted_tua_uuid\", \"answer_uuid\"]]\n",
    "# ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tua_uuid can be used as a unique identifier!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**can't use article_filename, since the same article can have multiple questions!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**the same article can have different tasks (quiz_task_uuid) and different contributors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # let's see article filename\n",
    "# ques_two_b[\"article_filename\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # right now I will use different articles same question as an example\n",
    "# # might be wrong\n",
    "# # select tua_uuid as different observers \n",
    "# pre = ques_two_b.loc[:, [\"quiz_task_uuid\", \"contributor_uuid\", \"question_label\", \"answer_label\"]]\n",
    "# pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the **reliability data matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert the df to pivot_table \n",
    "# pv = pd.pivot_table(data=pre, index=\"contributor_uuid\", columns=\"quiz_task_uuid\", values=\"answer_label\", aggfunc=list)\n",
    "# pv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geez, a lot of missing values! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**construct a coincidence matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triager to uAlpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping for topic_name\n",
    "dic = {\"Quoted Sources\":1,\n",
    "           \"Arguments\":2,\n",
    "           \"Assertions\":3, \n",
    "           \"Needs Fact-Check\":4, \n",
    "           \"Language\":5, \n",
    "           \"Reasoning\":6, \n",
    "           \"Probability\":7, \n",
    "           \"Evidence\":8}\n",
    "\n",
    "    \n",
    "# mapping topic_name to integers \n",
    "def mapping(df, dic):\n",
    "    # df - dataframe to be mapped \n",
    "    # dic - dictionary of mapping \n",
    "    df[\"topic_name\"] = df[\"topic_name\"].map(dic)\n",
    "    return df \n",
    "\n",
    "# add adjusted start_pos and adjusted end_pos for df \n",
    "def adjust(df):\n",
    "    # df - a dataframe with [\"article_number\", \"article_text_length\", \"start_pos\", \"end_pos\"] cols\n",
    "    article_number = df[\"article_number\"]\n",
    "    article_text_length = df[\"article_text_length\"]\n",
    "    start_pos = df[\"start_pos\"]\n",
    "    end_pos = df[\"end_pos\"]\n",
    "    # the cumulative sum of article_text_length\n",
    "    cumulative = []\n",
    "    start_pos_adjusted = []\n",
    "    end_pos_adjusted = []\n",
    "    pre_total = 0\n",
    "    total = article_text_length[0]\n",
    "    cur_num = article_number[0]\n",
    "    for i in np.arange(len(article_number)):\n",
    "        if article_number[i] == cur_num:\n",
    "            total = total + 0\n",
    "            cumulative.append(total)\n",
    "        else:\n",
    "            pre_total = total\n",
    "            cur_num = article_number[i]\n",
    "            total = total + article_text_length[i]\n",
    "            cumulative.append(total)\n",
    "        start_pos_adjusted.append(start_pos[i] + pre_total)\n",
    "        end_pos_adjusted.append(end_pos[i] + pre_total)\n",
    "    df[\"cumulative\"] = cumulative\n",
    "    df[\"start_pos_adjusted\"] = start_pos_adjusted\n",
    "    df[\"end_pos_adjusted\"] = end_pos_adjusted\n",
    "    return df \n",
    "\n",
    "\n",
    "# filter df and select only certain cols as input\n",
    "def slice_input(df):\n",
    "    return df.loc[:, [\"contributor_uuid\", \"topic_name\", \"start_pos\"\n",
    "                                    , \"end_pos\"\n",
    "                                    , \"article_text_length\"\n",
    "                                    , \"article_number\"]]\n",
    "\n",
    "\n",
    "# filter df and select only certain cols as output\n",
    "def slice_output(df):\n",
    "    return df.loc[:, [\"index\", \"contributor_uuid\", \n",
    "                               \"topic_name\", \n",
    "                               \"blank\", \n",
    "                               \"start_pos_adjusted\", \n",
    "                               \"end_pos_adjusted\", \n",
    "                               \"cumulative\"]]\n",
    "    \n",
    "\n",
    "# read in a directory that contains Triager csvs \n",
    "# split each Triager csv to 4 other csvs based on topic_name (argument, reasoning, etc)\n",
    "# and write it as csvs in the output directory\n",
    "def triager_split(input_directory, output_directory):\n",
    "    # input_directory - e.g. \"Triager data\"\n",
    "    # output_directory - e.g. \"Triager output\"\n",
    "    for file in os.listdir(input_directory):\n",
    "        triager_split_help(input_directory, file, output_directory)\n",
    "    print(\"Triager Tranformation Done! Ready to be imported to uAlpha!\")\n",
    "    \n",
    "# take in a Triager csv and split it to 4 different csvs \n",
    "# based on topic_name \n",
    "def triager_split_help(input_directory, fileName, output_directory):\n",
    "    # input_directory - e.g. \"Triager data\"\n",
    "    # fileName - e.g. \"Covid_Form1.0.adjudicated-2020-10-04T2314-Tags.csv\"\n",
    "    # output_directory - e.g. \"Triager output\"\n",
    "    name = os.path.join(input_directory, fileName)\n",
    "    df = pd.read_csv(name)\n",
    "    # different topic_names \n",
    "    topic_names = df[\"topic_name\"].unique()\n",
    "    for topic in topic_names: \n",
    "        # select all rows for that specific topic name \n",
    "        df_topic = df.loc[df[\"topic_name\"] == topic]\n",
    "        # re-index such that its index starts from 0 again\n",
    "        df_topic.reset_index(drop=True, inplace=True)\n",
    "        # select only these columns \n",
    "        filtered = slice_input(df_topic)\n",
    "        # slice contributor_uuid with the first 6 chars \n",
    "        filtered[\"contributor_uuid\"] = filtered[\"contributor_uuid\"].str[:6]\n",
    "        # map topic_name to integers\n",
    "        mapping(filtered, dic)\n",
    "        # add one column of u + str(row)\n",
    "        index = 'u' + pd.Series(filtered.index).astype(str)\n",
    "        filtered.insert(loc=0, column=\"index\", value=index)\n",
    "        # add a blank column \n",
    "        filtered.insert(loc=3, column=\"blank\", value=\"\")\n",
    "        # add modified columns\n",
    "        filtered = adjust(filtered)\n",
    "        # filter and select only certain columns \n",
    "        output = slice_output(filtered)\n",
    "        # the batch_name, e.g. \"Covid\"\n",
    "        batch_name = re.split(r'_', fileName)[0]\n",
    "        name = '{0}-Triager-{1}.csv'.format(batch_name, topic)\n",
    "        path = os.path.join(output_directory, name)\n",
    "        # write to csvs\n",
    "        output.to_csv(path)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triager Tranformation Done! Ready to be imported to uAlpha!\n"
     ]
    }
   ],
   "source": [
    "triager_split(\"Triager data\", \"Triager output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
